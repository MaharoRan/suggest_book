import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, precision_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer

print("Chargement du mod√®le SentenceTransformer (SBERT)...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# Charger le dataset
dataset = pd.read_csv('Book_Dataset_1.csv', sep=',', encoding='latin1')

dataset = dataset.drop_duplicates()

# Supprimer les livres avec cat√©gorie "Default", "add a comment" ou vide
dataset = dataset[
    (dataset['Category'].str.strip() != '') &
    (dataset['Category'].str.lower() != 'default')&
    (dataset['Category'].str.lower() != 'add a comment')
]

print(dataset.Title.count(), "livres apr√®s nettoyage des cat√©gories.")

# NOUVEAU: Filtrer les cat√©gories avec trop peu d'exemples
def clean_text(text):
    if pd.isna(text):
        return ''
    return str(text).lower().strip()

dataset['category_clean'] = dataset['Category'].apply(clean_text)

# Analyser la distribution
category_counts = dataset['category_clean'].value_counts()
print(f"\nNombre de cat√©gories uniques: {len(category_counts)}")

# Garder seulement les cat√©gories avec au moins 10 livres
min_books_per_category = 10
valid_categories = category_counts[category_counts >= min_books_per_category].index
dataset = dataset[dataset['category_clean'].isin(valid_categories)]

print(f"Filtrage: gard√© {len(valid_categories)} cat√©gories avec ‚â• {min_books_per_category} livres")
print(f"Total de livres apr√®s filtrage: {len(dataset)}")

print("\nDistribution des cat√©gories retenues:")
for category, count in dataset['category_clean'].value_counts().head(10).items():
    print(f"  - {category}: {count} livres")

# Nettoyer et pr√©parer le texte pour embedding
dataset = dataset.reset_index(drop=True)  # R√©initialiser les index

def clean_text(text):
    if pd.isna(text):
        return ''
    return str(text).lower().strip()
    if pd.isna(text):
        return ''
    return str(text).lower().strip()

dataset['title_clean'] = dataset['Title'].apply(clean_text)
dataset['category_clean'] = dataset['Category'].apply(clean_text)
dataset['description_clean'] = dataset['Book_Description'].apply(clean_text)

# Regrouper les champs pour l'embedding
dataset['text_for_embedding'] = (
    dataset['title_clean'] + ' ' +
    dataset['category_clean'] + ' ' +
    dataset['description_clean']
)

# Cr√©er embeddings avec SentenceTransformer
print("G√©n√©ration des embeddings SentenceTransformer (384 dimensions)...")
embeddings = model.encode(
    dataset['text_for_embedding'].tolist(),
    convert_to_tensor=True
)

# Convertir en numpy array pour sklearn
X = embeddings.cpu().numpy()
y = dataset['category_clean']

print(f"Forme de X: {X.shape}")
print(f"Dimension des embeddings: {X.shape[1]}")

# Normaliser les donn√©es
scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Mod√®le 1: Random Forest
print("\n" + "="*80)
print("√âVALUATION: Random Forest Classifier")
print("="*80)
model_rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_precision = precision_score(y_test, y_pred_rf, average='weighted', zero_division=0)
rf_f1 = f1_score(y_test, y_pred_rf, average='weighted', zero_division=0)
print(f"\nüìä R√©sultats Random Forest:")
print(f"   Accuracy: {rf_accuracy:.4f} | Precision: {rf_precision:.4f} | F1: {rf_f1:.4f}")

# Rapport d√©taill√© (top/bottom classes)
print("\nTop 5 cat√©gories les mieux pr√©dites:")
report = classification_report(y_test, y_pred_rf, output_dict=True, zero_division=0)
class_f1 = [(k, v['f1-score']) for k, v in report.items() if k not in ['accuracy', 'macro avg', 'weighted avg']]
class_f1_sorted = sorted(class_f1, key=lambda x: x[1], reverse=True)
for cat, f1 in class_f1_sorted[:5]:
    print(f"   {cat[:30]:30} F1: {f1:.3f}")

# =============================================================================
# APPROCHE S√âMANTIQUE PURE (sans ML classique)
# =============================================================================

print("\n" + "="*80)
print("√âVALUATION: Approche S√©mantique Pure (Cosine Similarity)")
print("="*80)

# Cr√©er les profils de cat√©gories AVANT de s√©parer train/test
X_all = embeddings.cpu().numpy()
X_all = scaler.fit_transform(X_all)
y_all = dataset['category_clean']

# Cr√©er des indices pour train/test
from sklearn.model_selection import train_test_split
indices = np.arange(len(dataset))
idx_train, idx_test = train_test_split(indices, test_size=0.3, random_state=42, stratify=y_all)

# Cr√©er les profils de cat√©gories bas√©s UNIQUEMENT sur le train set
category_profiles_semantic = {}
for category in dataset.iloc[idx_train]['category_clean'].unique():
    category_mask = dataset.iloc[idx_train]['category_clean'] == category
    train_indices_for_cat = idx_train[category_mask[idx_train]]
    category_embeddings = X_all[train_indices_for_cat]
    category_profiles_semantic[category] = np.mean(category_embeddings, axis=0)

# Pr√©dire sur le test set
y_pred_semantic = []
for idx in idx_test:
    query_embedding = X_all[idx].reshape(1, -1)
    
    # Calculer similarit√© avec chaque profil
    similarities = {}
    for category, profile in category_profiles_semantic.items():
        similarity = cosine_similarity(query_embedding, profile.reshape(1, -1))[0][0]
        similarities[category] = similarity
    
    # Pr√©dire la cat√©gorie la plus similaire
    predicted_category = max(similarities.items(), key=lambda x: x[1])[0]
    y_pred_semantic.append(predicted_category)

# √âvaluer
y_test_semantic = dataset.iloc[idx_test]['category_clean'].values
sem_accuracy = accuracy_score(y_test_semantic, y_pred_semantic)
sem_precision = precision_score(y_test_semantic, y_pred_semantic, average='weighted', zero_division=0)
sem_f1 = f1_score(y_test_semantic, y_pred_semantic, average='weighted', zero_division=0)

print(f"\nüìä R√©sultats Approche S√©mantique:")
print(f"   Accuracy: {sem_accuracy:.4f} | Precision: {sem_precision:.4f} | F1: {sem_f1:.4f}")

print("\n" + "="*80)
print("COMPARAISON DES APPROCHES")
print("="*80)
print(f"{'M√©thode':<30} {'Accuracy':<12} {'Precision':<12} {'F1-Score':<12}")
print("-"*80)
print(f"{'Random Forest':<30} {rf_accuracy:<12.4f} {rf_precision:<12.4f} {rf_f1:<12.4f}")
print(f"{'S√©mantique (Cosine)':<30} {sem_accuracy:<12.4f} {sem_precision:<12.4f} {sem_f1:<12.4f}")

improvement = ((sem_accuracy - rf_accuracy) / rf_accuracy) * 100 if rf_accuracy > 0 else 0
print(f"\n{'Am√©lioration s√©mantique:':<30} {improvement:+.2f}%")

# =============================================================================
# ANALYSE S√âMANTIQUE CONTEXTUELLE
# =============================================================================

print("\n" + "="*80)
print("ANALYSE S√âMANTIQUE ET CONTEXTUELLE")
print("="*80)

# 1. Cr√©er des profils de cat√©gories bas√©s sur les embeddings moyens (sur tout le dataset)
print("\n1. Cr√©ation des profils s√©mantiques par cat√©gorie...")
category_profiles = {}
for category in dataset['category_clean'].unique():
    category_mask = dataset['category_clean'] == category
    category_indices = np.where(category_mask)[0]
    category_embeddings = X_all[category_indices]
    # Profil = centro√Øde des embeddings de cette cat√©gorie
    category_profiles[category] = np.mean(category_embeddings, axis=0)

print(f"   ‚Üí {len(category_profiles)} profils de cat√©gories cr√©√©s")

# 2. Fonction de recherche s√©mantique par similarit√© cosine
def find_similar_books_by_text(query_text, top_k=5):
    """
    Recherche de livres similaires bas√©e sur la similarit√© cosine
    """
    # G√©n√©rer l'embedding de la requ√™te
    query_embedding = model.encode([query_text], convert_to_tensor=True).cpu().numpy()
    query_embedding = scaler.transform(query_embedding)
    
    # Calculer la similarit√© cosine avec tous les livres
    similarities = cosine_similarity(query_embedding, X_all)[0]
    
    # Trouver les top_k livres les plus similaires
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            'title': dataset.iloc[idx]['Title'],
            'category': dataset.iloc[idx]['Category'],
            'similarity_score': similarities[idx],
            'description': dataset.iloc[idx]['Book_Description'][:200] if pd.notna(dataset.iloc[idx]['Book_Description']) else 'N/A'
        })
    
    return results

# 3. Fonction de pr√©diction de cat√©gorie par similarit√© s√©mantique
def predict_category_semantic(query_text):
    """
    Pr√©dit la cat√©gorie d'un texte en utilisant la similarit√© cosine
    avec les profils de cat√©gories
    """
    # G√©n√©rer l'embedding de la requ√™te
    query_embedding = model.encode([query_text], convert_to_tensor=True).cpu().numpy()
    query_embedding = scaler.transform(query_embedding)
    
    # Calculer la similarit√© avec chaque profil de cat√©gorie
    similarities = {}
    for category, profile in category_profiles.items():
        similarity = cosine_similarity(query_embedding, profile.reshape(1, -1))[0][0]
        similarities[category] = similarity
    
    # Trier par similarit√© d√©croissante
    sorted_categories = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_categories

# 4. Clustering s√©mantique des livres
print("\n2. Clustering s√©mantique des livres...")
n_clusters = min(10, len(dataset['category_clean'].unique()))
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_all)
dataset['semantic_cluster'] = cluster_labels

print(f"   ‚Üí {n_clusters} clusters cr√©√©s")
print("\nDistribution des clusters:")
cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
for cluster_id, count in cluster_counts.items():
    print(f"   Cluster {cluster_id}: {count} livres")

# 5. Exemples d'utilisation
print("\n" + "="*80)
print("EXEMPLES D'ANALYSE S√âMANTIQUE")
print("="*80)

# Exemple 1: Recherche s√©mantique
print("\nüìö Exemple 1: Recherche de livres similaires")
query_example = "artificial intelligence and machine learning"
print(f"Requ√™te: '{query_example}'")
similar_books = find_similar_books_by_text(query_example, top_k=3)
print("\nLivres les plus similaires:")
for i, book in enumerate(similar_books, 1):
    print(f"\n{i}. {book['title']}")
    print(f"   Cat√©gorie: {book['category']}")
    print(f"   Score de similarit√©: {book['similarity_score']:.4f}")
    print(f"   Description: {book['description']}...")

# Exemple 2: Pr√©diction de cat√©gorie par similarit√© s√©mantique
print("\n\nüè∑Ô∏è  Exemple 2: Pr√©diction de cat√©gorie par analyse s√©mantique")
query_example_2 = "detective mystery crime thriller investigation"
print(f"Requ√™te: '{query_example_2}'")
category_predictions = predict_category_semantic(query_example_2)
print("\nTop 5 cat√©gories les plus similaires:")
for i, (category, similarity) in enumerate(category_predictions[:5], 1):
    print(f"{i}. {category.title()}: {similarity:.4f}")

# Exemple 3: Analyse d'un cluster s√©mantique
print("\n\nüîç Exemple 3: Analyse d'un cluster s√©mantique")
cluster_to_analyze = 0
cluster_books = dataset[dataset['semantic_cluster'] == cluster_to_analyze]
print(f"Cluster {cluster_to_analyze} ({len(cluster_books)} livres):")
print("\nCat√©gories principales dans ce cluster:")
category_distribution = cluster_books['category_clean'].value_counts().head(5)
for category, count in category_distribution.items():
    print(f"   - {category.title()}: {count} livres")
print("\nExemples de titres:")
for title in cluster_books['Title'].head(3):
    print(f"   ‚Ä¢ {title}")

# 6. √âvaluation de la coh√©rence s√©mantique
print("\n\nüìä √âvaluation de la coh√©rence s√©mantique par cat√©gorie")
print("-" * 80)
for category in list(dataset['category_clean'].unique())[:5]:  # Top 5 cat√©gories
    category_books = dataset[dataset['category_clean'] == category]
    if len(category_books) > 1:
        category_indices = np.where(dataset['category_clean'] == category)[0]
        category_embeddings = X_all[category_indices]
        # Calculer la similarit√© intra-cat√©gorie moyenne
        intra_similarity = cosine_similarity(category_embeddings)
        # Moyenne des similarit√©s (excluant la diagonale)
        mask = ~np.eye(intra_similarity.shape[0], dtype=bool)
        avg_similarity = intra_similarity[mask].mean()
        print(f"{category.title()[:30]:30} | Coh√©rence: {avg_similarity:.4f} | Livres: {len(category_books)}")

print("\n" + "="*80)
print("‚úÖ Analyse s√©mantique contextuelle termin√©e!")
print("="*80)